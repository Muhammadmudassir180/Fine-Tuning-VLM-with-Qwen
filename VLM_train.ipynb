{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c667b53",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a33bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import unsloth\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from unsloth import FastVisionModel\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fcf84",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75796123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 81725it [00:01, 44968.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 65380, Eval: 8172, Test: 8173\n",
      "Sample train data:\n",
      " [{'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': \"You are highly advance Vision Language Model, specializing in analyzing, describing and interpreting medical images .You are given a medical image and a question related to it. Your task is to analyze, process and choose the most appropriate answer from the options provided.\\n Question: In arterial tortuosity syndrome, where does the distal aorta tend to extend as observed in the figure? \\n Options:\\n A. To the right pulmonary artery\\nB. To the left hemithorax\\nC. Toward the heart's apex\\nD. Upward towards the neck\\nE. Into the right atrium\\nF. Toward the diaphragm\"}, {'type': 'image', 'image': 'C:/Users/Hey-BUDD/Desktop/Deep Learining Project/Datasets/RadFig-VQA/imgs/imgs/PMC4168646/JCIS-4-44-g012.jpg'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 'B. To the left hemithorax'}]}]}, {'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'You are highly advance Vision Language Model, specializing in analyzing, describing and interpreting medical images .You are given a medical image and a question related to it. Your task is to analyze, process and choose the most appropriate answer from the options provided.\\n Question: What did the unenhanced emergency CT scans reveal in the figure of the 59-year-old female? \\n Options:\\n A. A perforated duodenal ulcer\\nB. A thickened bowel wall\\nC. Hyper attenuating content in the proximal duodenum\\nD. A large intraperitoneal hematoma\\nE. Free intraperitoneal air\\nF. Completely normal findings'}, {'type': 'image', 'image': 'C:/Users/Hey-BUDD/Desktop/Deep Learining Project/Datasets/RadFig-VQA/imgs/imgs/PMC5621988/13244_2017_562_Fig9_HTML.jpg'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 'C. Hyper attenuating content in the proximal duodenum'}]}]}, {'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'You are highly advance Vision Language Model, specializing in analyzing, describing and interpreting medical images .You are given a medical image and a question related to it. Your task is to analyze, process and choose the most appropriate answer from the options provided.\\n Question: What is the prognosis of the male patient aged 25 years with the radiological finding shown in the figure? \\n Options:\\n A. Poor\\nB. Moderate\\nC. Uncertain\\nD. Good\\nE. Severe\\nF. Not applicable'}, {'type': 'image', 'image': 'C:/Users/Hey-BUDD/Desktop/Deep Learining Project/Datasets/RadFig-VQA/imgs/imgs/PMC8727045/43055_2021_685_Fig16_HTML.jpg'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 'D. Good'}]}]}, {'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'You are highly advance Vision Language Model, specializing in analyzing, describing and interpreting medical images .You are given a medical image and a question related to it. Your task is to analyze, process and choose the most appropriate answer from the options provided.\\n Question: What is observed in the bilateral testes on the follow-up transverse testicular ultrasound after 3 months? \\n Options:\\n A. Increase in the size of the masses\\nB. The masses have disappeared\\nC. Change in echogenicity of the masses\\nD. Unchanged size of the mixed echogenicity masses\\nE. New masses appearing in addition to the existing ones\\nF. No abnormalities detected'}, {'type': 'image', 'image': 'C:/Users/Hey-BUDD/Desktop/Deep Learining Project/Datasets/RadFig-VQA/imgs/imgs/PMC8718818/gr8.jpg'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 'D. Unchanged size of the mixed echogenicity masses'}]}]}, {'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'You are highly advance Vision Language Model, specializing in analyzing, describing and interpreting medical images .You are given a medical image and a question related to it. Your task is to analyze, process and choose the most appropriate answer from the options provided.\\n Question: What is the purpose of the surgical approach shown in the figure after a central pancreatectomy? \\n Options:\\n A. To remove the remnant pancreas head\\nB. To ensure stable biliary drainage and functional preservation of the pancreas\\nC. To obstruct the biliary ducts\\nD. To resect the proximal pancreas\\nE. To transition the patient off enzyme supplements\\nF. To completely excise the pancreas'}, {'type': 'image', 'image': 'C:/Users/Hey-BUDD/Desktop/Deep Learining Project/Datasets/RadFig-VQA/imgs/imgs/PMC5313518/kjr-18-299-g008.jpg'}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 'B. To ensure stable biliary drainage and functional preservation of the pancreas'}]}]}]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "json_path=r\"./vlm_dataset.jsonl\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json \n",
    "from tqdm import tqdm\n",
    "import random\n",
    "#for Train, Eval and Test split\n",
    "# dataset_train=load_dataset(\"json\", data_files=json_path,split=\"train[20%:]\")\n",
    "# print(dataset_train[1:5])\n",
    "# print(len(dataset_train))\n",
    "\n",
    "\n",
    "\n",
    "# dataset_eval=load_dataset(\"json\", data_files=json_path,split=\"train[:10%]\")\n",
    "# print(dataset_eval[1:5])\n",
    "# print(len(dataset_eval))\n",
    "\n",
    "# dataset_test=load_dataset(\"json\", data_files=json_path,split=\"train[10%:20%]\")\n",
    "# print(dataset_test[1:5])\n",
    "# print(len(dataset_test))\n",
    "# dataset_train, dataset_eval=load_dataset(\"json\", data_files=json_path,split=\"train\").train_test_split(test_size=0.2).values()\n",
    "# dataset_train[1:5]\n",
    "# print(len(dataset))\n",
    "# train, test = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "EVAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in tqdm(f, desc=\"Loading dataset\")]\n",
    "random.seed(42)\n",
    "random.shuffle(data)\n",
    "\n",
    "n_total = len(data)\n",
    "n_train = int(n_total * TRAIN_RATIO)\n",
    "n_eval = int(n_total * EVAL_RATIO)\n",
    "\n",
    "train_data = data[:n_train]\n",
    "eval_data = data[n_train:n_train + n_eval]\n",
    "test_data = data[n_train + n_eval:]\n",
    "with open(\"test_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in test_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "with open(\"eval_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in eval_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "with open(\"train_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "print(f\"Train: {len(train_data)}, Eval: {len(eval_data)}, Test: {len(test_data)}\")\n",
    "print(\"Sample train data:\\n\", train_data[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc9561",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa6da13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, start=1):\n",
    "        data = json.loads(line)\n",
    "        user_contents = data[\"messages\"][0][\"content\"]\n",
    "        images = [x for x in user_contents if x[\"type\"] == \"image\"]\n",
    "        if not images or not images[0][\"image\"]:\n",
    "            print(f\"⚠️ Missing image at line {i}\")\n",
    "import json, re\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        data = json.loads(line)\n",
    "        text = \" \".join([x[\"text\"] for x in data[\"messages\"][0][\"content\"] if x[\"type\"] == \"text\"])\n",
    "        if re.search(r\"<\\|?image\", text):\n",
    "            print(f\"⚠️ Found image placeholder in text at line {i}\")\n",
    "\n",
    "import json\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f, 1):\n",
    "        data = json.loads(line)\n",
    "        imgs = [c for m in data[\"messages\"] for c in m[\"content\"] if c[\"type\"] == \"image\"]\n",
    "        if len(imgs) > 1:\n",
    "            print(f\"⚠️ Multiple images at line {i}: {[x['image'] for x in imgs]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device does not support bfloat16. Will change to float16.\n",
      "c:\\Users\\Hey-BUDD\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:341: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE}:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.1: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2060 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel \n",
    "model,tokenizer=FastVisionModel.from_pretrained(\"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\",\n",
    "    max_seq_length=4096,\n",
    "    dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=\"unsloth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True,\n",
    "    finetune_language_layers   = True, #\n",
    "    finetune_attention_modules = True, \n",
    "    finetune_mlp_modules       = True,\n",
    "\n",
    "    r = 16,            \n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e60c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "Both warmup_ratio and warmup_steps given, warmup_steps will override any effect of warmup_ratio during training\n"
     ]
    }
   ],
   "source": [
    "output=\"./outputs/Qwen2.5-VL-3B-Instruct-bnb-4bit-finetuned-r10\"\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "        # per_device_train_batch_size = 8,\n",
    "        # gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 100,\n",
    "        max_steps = 4000,\n",
    "        # num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-5,\n",
    "        logging_steps = 50,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = output,\n",
    "        report_to = \"none\",     # For Weights and Biases\n",
    "        # eval_strategy = \"steps\",\n",
    "        metric_for_best_model = \"loss\",\n",
    "        # load_best_model_at_end = True,\n",
    "        greater_is_better = False,\n",
    "        log_level = \"info\",\n",
    "        \n",
    "        save_strategy =\"steps\",\n",
    "        save_steps = 100,\n",
    "        max_grad_norm = 1.0,    \n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        max_length = 4096,      \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d72a203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    }
   ],
   "source": [
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "\n",
    "FastVisionModel.for_training(model)\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
    "    \n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = eval_data,     \n",
    "    args = args\n",
    "    \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a80b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 2060 SUPER. Max memory = 8.0 GB.\n",
      "11.533 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f840676",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train(resume_from_checkpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69f4cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abdominal ultrasound (US) was used. The figure focuses on the lesion in the upper left part of the right adrenal gland, measured with a diameter of approximately 13mm in diameter. The right adrenal gland also contains a malignant lesion, approximately 10×9 mm2. These findings were confirmed during surgery. Additionally, the lower left adrenals contain several small benign and malignant masses, as well as cystic structures in both adrenal glands. There was no evidence of metastasis in lymph nodes. The lesions were located within the adrenal glands according to imaging and post-operative histological examination. The largest benign nodule in\n"
     ]
    }
   ],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "image = \"C:/Users/Hey-BUDD/Desktop/Deep Learining Project/Datasets/RadFig-VQA/imgs/imgs/PMC6604293/12880_2019_349_Fig1_HTML.jpg\"\n",
    "question = \"What imaging technique is used to display the lesions shown in the figure?\"\n",
    "\n",
    "instruction = \"You are an expert radiographer. Describe accurately what you see in this image.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction+\"\\n\"+question}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Hey-BUDD\\.cache\\huggingface\\hub\\models--unsloth--qwen2.5-vl-3b-instruct-bnb-4bit\\snapshots\\f9adca15cefb612e7f03a1c8a5fddb920b1786f3\\config.json\n",
      "Model config Qwen2_5_VLConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2_5_VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 128000,\n",
      "  \"max_window_layers\": 70,\n",
      "  \"model_type\": \"qwen2_5_vl\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"pad_token_id\": 151654,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": [\n",
      "      \"lm_head\",\n",
      "      \"multi_modal_projector\",\n",
      "      \"merger\",\n",
      "      \"modality_projection\"\n",
      "    ],\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"text_config\": {\n",
      "    \"architectures\": [\n",
      "      \"Qwen2_5_VLForConditionalGeneration\"\n",
      "    ],\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"dtype\": \"bfloat16\",\n",
      "    \"eos_token_id\": 151645,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 2048,\n",
      "    \"image_token_id\": null,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 11008,\n",
      "    \"layer_types\": [\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\"\n",
      "    ],\n",
      "    \"max_position_embeddings\": 128000,\n",
      "    \"max_window_layers\": 70,\n",
      "    \"model_type\": \"qwen2_5_vl_text\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_hidden_layers\": 36,\n",
      "    \"num_key_value_heads\": 2,\n",
      "    \"pad_token_id\": 151654,\n",
      "    \"quantization_config\": {\n",
      "      \"_load_in_4bit\": true,\n",
      "      \"_load_in_8bit\": false,\n",
      "      \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "      \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "      \"bnb_4bit_quant_type\": \"nf4\",\n",
      "      \"bnb_4bit_use_double_quant\": true,\n",
      "      \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "      \"llm_int8_has_fp16_weight\": false,\n",
      "      \"llm_int8_skip_modules\": [\n",
      "        \"lm_head\",\n",
      "        \"multi_modal_projector\",\n",
      "        \"merger\",\n",
      "        \"modality_projection\"\n",
      "      ],\n",
      "      \"llm_int8_threshold\": 6.0,\n",
      "      \"load_in_4bit\": true,\n",
      "      \"load_in_8bit\": false,\n",
      "      \"quant_method\": \"bitsandbytes\"\n",
      "    },\n",
      "    \"rms_norm_eps\": 1e-06,\n",
      "    \"rope_scaling\": {\n",
      "      \"mrope_section\": [\n",
      "        16,\n",
      "        24,\n",
      "        24\n",
      "      ],\n",
      "      \"rope_type\": \"default\",\n",
      "      \"type\": \"default\"\n",
      "    },\n",
      "    \"rope_theta\": 1000000.0,\n",
      "    \"sliding_window\": null,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"unsloth_fixed\": true,\n",
      "    \"use_cache\": true,\n",
      "    \"use_sliding_window\": false,\n",
      "    \"video_token_id\": null,\n",
      "    \"vision_end_token_id\": 151653,\n",
      "    \"vision_start_token_id\": 151652,\n",
      "    \"vision_token_id\": 151654,\n",
      "    \"vocab_size\": 151936\n",
      "  },\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"unsloth_fixed\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"depth\": 32,\n",
      "    \"dtype\": \"bfloat16\",\n",
      "    \"fullatt_block_indexes\": [\n",
      "      7,\n",
      "      15,\n",
      "      23,\n",
      "      31\n",
      "    ],\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 1280,\n",
      "    \"in_channels\": 3,\n",
      "    \"in_chans\": 3,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3420,\n",
      "    \"model_type\": \"qwen2_5_vl\",\n",
      "    \"num_heads\": 16,\n",
      "    \"out_hidden_size\": 2048,\n",
      "    \"patch_size\": 14,\n",
      "    \"spatial_merge_size\": 2,\n",
      "    \"spatial_patch_size\": 14,\n",
      "    \"temporal_patch_size\": 2,\n",
      "    \"tokens_per_second\": 2,\n",
      "    \"window_size\": 112\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "Image processor saved in Qwen_25_VL_Finetuned_RadFigv2\\preprocessor_config.json\n",
      "Video processor saved in Qwen_25_VL_Finetuned_RadFigv2\\video_preprocessor_config.json\n",
      "chat template saved in Qwen_25_VL_Finetuned_RadFigv2\\chat_template.jinja\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"Qwen_25_VL_Finetuned_RadFigv2\")  # Local saving\n",
    "tokenizer.save_pretrained(\"Qwen_25_VL_Finetuned_RadFigv2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7640982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
